{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image to Video Generation: A Demo Notebook\n",
        "\n",
        "This Jupyter Notebook provides a runnable implementation of the end-to-end **Video Virtual Try-On (VVT)** pipeline detailed in the provided technical report, *'Architecting a Web-Based Video Virtual Try-On Demonstrator'*. \n",
        "\n",
        "The notebook orchestrates a series of state-of-the-art open-source models to take a video of a person and an image of a garment, and produce a new video of that person wearing the new garment. The core generative model used is **ViViD**, as selected in the report.\n",
        "\n",
        "### ⚠️ **System Requirements**\n",
        "This is a computationally intensive pipeline that requires specific hardware and significant storage for model weights.\n",
        "- **GPU**: A powerful NVIDIA GPU with **at least 24 GB of VRAM** is strongly recommended. The process may fail on GPUs with less memory.\n",
        "- **RAM**: 32 GB of system RAM or more.\n",
        "- **Storage**: ~50 GB of free disk space for repositories and model checkpoints.\n",
        "- **OS**: This notebook is intended for a Linux environment with `git` and `wget` installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup\n",
        "\n",
        "First, we clone the necessary open-source repositories. The pipeline is a complex system that relies on code from multiple projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Cloning required repositories...\")\n",
        "# Clone the main ViViD repository for the core VTON model\n",
        "!git clone https://github.com/alibaba-yuanjing-aigclab/ViViD.git\n",
        "\n",
        "# Clone vid2densepose for human pose extraction\n",
        "!git clone https://github.com/Flode-Labs/vid2densepose.git\n",
        "\n",
        "# Clone Segment Anything Model (SAM) for garment masking\n",
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "# Clone OOTDiffusion and its recommended human parser for the clothing-agnostic step\n",
        "!git clone https://github.com/levihsu/OOTDiffusion.git\n",
        "!git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git\n",
        "print(\"Repositories cloned successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we install all the Python dependencies from the cloned repositories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%pip install -r ViViD/requirements.txt\n",
        "%pip install -r vid2densepose/requirements.txt\n",
        "# Install Detectron2, a dependency for vid2densepose\n",
        "%pip install \"git+https://github.com/facebookresearch/detectron2.git\"\n",
        "# Install Segment Anything\n",
        "%pip install -e segment-anything/\n",
        "# Install OOTDiffusion dependencies\n",
        "%pip install -r OOTDiffusion/requirements.txt\n",
        "# Install additional required libraries\n",
        "%pip install pyyaml\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Pre-trained Model Weights\n",
        "\n",
        "This pipeline requires several large pre-trained model files (checkpoints). We will create directories for them and then download the files.\n",
        "\n",
        "**Note:** Some download links, particularly for the main ViViD model, may change. Always refer to the official GitHub repositories for the most up-to-date links if a download fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Creating directories for checkpoints...\")\n",
        "os.makedirs(\"ViViD/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"vid2densepose/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"segment-anything/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"OOTDiffusion/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"Self-Correction-Human-Parsing/checkpoints\", exist_ok=True)\n",
        "\n",
        "print(\"Downloading model weights...\")\n",
        "\n",
        "# --- ViViD Checkpoints ---\n",
        "# Please download the ViViD checkpoints from their official repository or Hugging Face page\n",
        "# and place them in the 'ViViD/checkpoints/' directory.\n",
        "print(\"ACTION REQUIRED: Manually download ViViD weights into ViViD/checkpoints/\")\n",
        "\n",
        "# --- vid2densepose Checkpoint ---\n",
        "# This is the model for DensePose extraction.\n",
        "!wget -P vid2densepose/checkpoints/ https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
        "\n",
        "# --- Segment Anything (SAM) Checkpoint ---\n",
        "# Using the ViT-H SAM model.\n",
        "!wget -P segment-anything/checkpoints/ https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "# --- Human Parsing Checkpoint (for Agnostic Video) ---\n",
        "# This model is used by OOTDiffusion's preprocessing steps.\n",
        "!wget -P Self-Correction-Human-Parsing/checkpoints/ https://github.com/GoGoDuck912/Self-Correction-Human-Parsing/releases/download/eccv2022/exp-schp-201908261155-lip.pth\n",
        "\n",
        "print(\"\\nDownloads complete. Please ensure you manually downloaded the ViViD weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configuration\n",
        "\n",
        "Set the paths for your input files and the directory for the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- USER INPUTS ---\n",
        "\n",
        "# Path to the input video of the person\n",
        "# IMPORTANT: The person video should be a short clip (e.g., 5-10 seconds).\n",
        "person_video_path = \"./path/to/your/person_video.mp4\" \n",
        "\n",
        "# Path to the input image of the garment\n",
        "garment_image_path = \"./path/to/your/garment_image.png\"\n",
        "\n",
        "# Directory to save all intermediate and final results\n",
        "output_dir = \"./vton_results\"\n",
        "\n",
        "# --- END USER INPUTS ---\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define paths for all intermediate files\n",
        "densepose_video_path = os.path.join(output_dir, \"densepose.mp4\")\n",
        "garment_mask_path = os.path.join(output_dir, \"garment_mask.png\")\n",
        "agnostic_video_path = os.path.join(output_dir, \"agnostic.mp4\")\n",
        "final_video_path = os.path.join(output_dir, \"result.mp4\")\n",
        "\n",
        "print(f\"Input Person Video: {person_video_path}\")\n",
        "print(f\"Input Garment Image: {garment_image_path}\")\n",
        "print(f\"Output Directory: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: The VTON Pipeline\n",
        "Here, we define Python functions to encapsulate each stage of the pipeline described in the report. This automates the series of command-line calls and script executions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import yaml\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "def generate_densepose_video(input_video, output_video):\n",
        "    \"\"\"Runs the vid2densepose script to extract human pose.\"\"\"\n",
        "    print(\"\\n--- Stage 1: Generating DensePose Video ---\")\n",
        "    vid2densepose_script = \"vid2densepose/main.py\"\n",
        "    # Note: vid2densepose needs its own checkpoint for the detectron2 model.\n",
        "    # The script is hardcoded to use a specific model, so we don't need to pass the path.\n",
        "    command = [\n",
        "        \"python\", vid2densepose_script,\n",
        "        \"-i\", input_video,\n",
        "        \"-o\", output_video\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(\"✅ DensePose video generated successfully.\")\n",
        "\n",
        "def generate_garment_mask(input_image, output_mask):\n",
        "    \"\"\"Uses Segment Anything Model (SAM) to isolate the garment.\"\"\"\n",
        "    print(\"\\n--- Stage 2: Generating Garment Mask ---\")\n",
        "    sam_checkpoint = \"segment-anything/checkpoints/sam_vit_h_4b8939.pth\"\n",
        "    model_type = \"vit_h\"\n",
        "    device = \"cuda\"\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    sam.to(device=device)\n",
        "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "    image = cv2.imread(input_image)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image file: {input_image}\")\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    masks = mask_generator.generate(image_rgb)\n",
        "\n",
        "    # Find the best mask - typically the one with the largest area that is centrally located.\n",
        "    if not masks:\n",
        "        raise ValueError(\"SAM did not find any objects in the garment image.\")\n",
        "    \n",
        "    # Simple logic: select the largest mask\n",
        "    sorted_masks = sorted(masks, key=lambda m: m['area'], reverse=True)\n",
        "    best_mask = sorted_masks[0]['segmentation']\n",
        "    \n",
        "    # Create a binary image (0 or 255) and save it\n",
        "    binary_mask_img = (best_mask * 255).astype(np.uint8)\n",
        "    cv2.imwrite(output_mask, binary_mask_img)\n",
        "    print(\"✅ Garment mask generated successfully.\")\n",
        "\n",
        "def generate_agnostic_video(input_video, output_video):\n",
        "    \"\"\"\n",
        "    Generates the clothing-agnostic video. \n",
        "    This is a complex step involving human parsing and inpainting.\n",
        "    The ViViD paper recommends the method from OOTDiffusion.\n",
        "    This function will call a helper script from the OOTDiffusion project.\n",
        "    *** NOTE: You may need to create or adapt a script in the OOTDiffusion \n",
        "    *** repository to perform this video-based task.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Stage 3: Generating Clothing-Agnostic Video ---\")\n",
        "    # This assumes a script exists at 'OOTDiffusion/run_agnostic_video.py'\n",
        "    # that takes an input video, uses the human_parser, and creates an inpainted video.\n",
        "    agnostic_script = \"OOTDiffusion/run_agnostic_video.py\" # You might need to create this script.\n",
        "    \n",
        "    if not os.path.exists(agnostic_script):\n",
        "        print(\"⚠️ WARNING: Agnostic video generation script not found.\")\n",
        "        print(f\"This step is being SKIPPED. You must implement '{agnostic_script}' using OOTDiffusion's human parsing and inpainting logic.\")\n",
        "        # As a fallback, we will just copy the original video.\n",
        "        # The final result will be poor, but it allows the pipeline to complete.\n",
        "        import shutil\n",
        "        shutil.copyfile(input_video, output_video)\n",
        "        return\n",
        "        \n",
        "    command = [\n",
        "        \"python\", agnostic_script,\n",
        "        \"--input_video\", input_video,\n",
        "        \"--output_video\", output_video,\n",
        "        \"--parser_checkpoint\", \"Self-Correction-Human-Parsing/checkpoints/exp-schp-201908261155-lip.pth\"\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(\"✅ Agnostic video generated successfully.\")\n",
        "\n",
        "def run_vivid_inference(config_path, person_video, densepose_video, garment_image, garment_mask, final_output_dir):\n",
        "    \"\"\"Runs the final ViViD model inference.\"\"\"\n",
        "    print(\"\\n--- Stage 4: Running ViViD Inference ---\")\n",
        "    \n",
        "    # 4a. Modify the ViViD config file dynamically\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # Update paths in the config\n",
        "    config['test_cases']['upper_1']['garment_path'] = os.path.abspath(garment_image)\n",
        "    config['test_cases']['upper_1']['garment_mask_path'] = os.path.abspath(garment_mask)\n",
        "    config['test_cases']['upper_1']['video_path'] = os.path.abspath(person_video) # The agnostic video path\n",
        "    config['test_cases']['upper_1']['pose_video'] = os.path.abspath(densepose_video)\n",
        "    config['out_dir'] = os.path.abspath(final_output_dir)\n",
        "    \n",
        "    # Save the modified config to a new file\n",
        "    modified_config_path = os.path.join(output_dir, \"run_config.yaml\")\n",
        "    with open(modified_config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    print(f\"Modified config saved to {modified_config_path}\")\n",
        "\n",
        "    # 4b. Run the inference script\n",
        "    vivid_inference_script = \"ViViD/vivid.py\"\n",
        "    command = [\n",
        "        \"python\", vivid_inference_script,\n",
        "        \"--config\", modified_config_path\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(f\"✅ Final video generated in directory: {final_output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Orchestration and Execution\n",
        "\n",
        "Now, we'll create a master function to run the entire pipeline in sequence and then execute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_full_pipeline():\n",
        "    \"\"\"Orchestrates the entire VTON pipeline from raw inputs to final video.\"\"\"\n",
        "    print(\"===========================================\")\n",
        "    print(\"STARTING VIDEO VIRTUAL TRY-ON PIPELINE\")\n",
        "    print(\"===========================================\")\n",
        "\n",
        "    try:\n",
        "        # Stage 1: DensePose Extraction\n",
        "        generate_densepose_video(person_video_path, densepose_video_path)\n",
        "\n",
        "        # Stage 2: Garment Masking\n",
        "        generate_garment_mask(garment_image_path, garment_mask_path)\n",
        "\n",
        "        # Stage 3: Clothing-Agnostic Video\n",
        "        # For this demo, we assume the agnostic video is the same as the original.\n",
        "        # Replace this with the real `generate_agnostic_video` call once implemented.\n",
        "        print(\"\\n--- Stage 3: Generating Clothing-Agnostic Video (SKIPPED) ---\")\n",
        "        print(\"Using original person video as the agnostic video. For best results, implement this step.\")\n",
        "        agnostic_input_video = person_video_path \n",
        "        # generate_agnostic_video(person_video_path, agnostic_video_path)\n",
        "        # agnostic_input_video = agnostic_video_path\n",
        "\n",
        "        # Stage 4: Run ViViD Inference\n",
        "        vivid_config_template = \"ViViD/configs/prompts/upper1.yaml\"\n",
        "        vivid_output_dir = os.path.join(output_dir, 'vivid_output')\n",
        "        run_vivid_inference(\n",
        "            config_path=vivid_config_template, \n",
        "            person_video=agnostic_input_video, \n",
        "            densepose_video=densepose_video_path, \n",
        "            garment_image=garment_image_path, \n",
        "            garment_mask=garment_mask_path, \n",
        "            final_output_dir=vivid_output_dir\n",
        "        )\n",
        "        \n",
        "        print(\"\\n===========================================\")\n",
        "        print(\"🎉 PIPELINE COMPLETED SUCCESSFULLY! 🎉\")\n",
        "        print(f\"Check the '{vivid_output_dir}' directory for your video.\")\n",
        "        print(\"===========================================\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\nERROR: A required file was not found. Please check your paths.\")\n",
        "        print(e)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\nERROR: A script failed to execute. See the output above for details.\")\n",
        "        print(e)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "\n",
        "# --- UNCOMMENT THE LINE BELOW TO RUN THE PIPELINE ---\n",
        "# run_full_pipeline()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
