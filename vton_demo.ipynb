{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image to Video Generation: A Demo Notebook\n",
        "\n",
        [span_0](start_span)"This Jupyter Notebook provides a runnable implementation of the end-to-end **Video Virtual Try-On (VVT)** pipeline detailed in the provided technical report, *'Architecting a Web-Based Video Virtual Try-On Demonstrator'*.[span_0](end_span)\n",
        "\n",
        [span_1](start_span)"The notebook orchestrates a series of state-of-the-art open-source models to take a video of a person and an image of a garment, and produce a new video of that person wearing the new garment.[span_1](end_span) [span_2](start_span)[span_3](start_span)The core generative model used is **ViViD**, as selected in the report.[span_2](end_span)[span_3](end_span)\n",
        "\n",
        "### ⚠️ **System Requirements**\n",
        [span_4](start_span)[span_5](start_span)"This is a computationally intensive pipeline that requires specific hardware and significant storage for model weights.[span_4](end_span)[span_5](end_span)\n",
        [span_6](start_span)"- **GPU**: A powerful NVIDIA GPU with **at least 16-24 GB of VRAM** is strongly recommended.[span_6](end_span) The process may fail on GPUs with less memory.\n",
        [span_7](start_span)"- **RAM**: 64 GB of system RAM or more is also crucial.[span_7](end_span)\n",
        [span_8](start_span)"- **Storage**: Significant free disk space (~50 GB) is required for repositories and model checkpoints.[span_8](end_span)\n",
        "- **OS**: This notebook is intended for a Linux environment with `git` and `wget` installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup\n",
        "\n",
        "First, we clone the necessary open-source repositories. [span_9](start_span)[span_10](start_span)The pipeline is a complex system that relies on code from multiple projects.[span_9](end_span)[span_10](end_span)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Cloning required repositories...\")\n",
        [span_11](start_span)"# Clone the main ViViD repository for the core VTON model[span_11](end_span)\n",
        "!git clone https://github.com/alibaba-yuanjing-aigclab/ViViD.git\n",
        "\n",
        [span_12](start_span)"# Clone vid2densepose for human pose extraction[span_12](end_span)\n",
        "!git clone https://github.com/Flode-Labs/vid2densepose.git\n",
        "\n",
        [span_13](start_span)"# Clone Segment Anything Model (SAM) for garment masking[span_13](end_span)\n",
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        [span_14](start_span)"# The clothing-agnostic step requires the methodology from OOTDiffusion[span_14](end_span)\n",
        [span_15](start_span)"# which in turn uses a human parsing model[span_15](end_span). We clone the necessary repositories.\n",
        "!git clone https://github.com/levihsu/OOTDiffusion.git\n",
        "!git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git\n",
        "\n",
        "print(\"Repositories cloned successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        [span_16](start_span)"Next, we install all the Python dependencies from the cloned repositories.[span_16](end_span)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%pip install -r ViViD/requirements.txt\n",
        "%pip install -r vid2densepose/requirements.txt\n",
        [span_17](start_span)"# Install Detectron2, a dependency for vid2densepose[span_17](end_span)\n",
        "%pip install \"git+https://github.com/facebookresearch/detectron2.git\"\n",
        [span_18](start_span)"# Install Segment Anything[span_18](end_span)\n",
        "%pip install -e segment-anything/\n",
        "# Install OOTDiffusion dependencies\n",
        "%pip install -r OOTDiffusion/requirements.txt\n",
        "# Install additional required libraries for scripting the pipeline\n",
        "%pip install pyyaml\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Pre-trained Model Weights\n",
        "\n",
        [span_19](start_span)[span_20](start_span)"This pipeline requires several large pre-trained model files (checkpoints).[span_19](end_span)[span_20](end_span) We will create directories for them and then download the files.\n",
        "\n",
        "**Note:** Some download links, particularly for the main ViViD model, may change. [span_21](start_span)Always refer to the official GitHub repositories for the most up-to-date links if a download fails.[span_21](end_span)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Creating directories for checkpoints...\")\n",
        "os.makedirs(\"ViViD/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"vid2densepose/model\", exist_ok=True) # vid2densepose expects a 'model' subdir\n",
        "os.makedirs(\"segment-anything/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"OOTDiffusion/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"Self-Correction-Human-Parsing/checkpoints\", exist_ok=True)\n",
        "\n",
        "print(\"Downloading model weights...\")\n",
        "\n",
        "# --- ViViD Checkpoints ---\n",
        "# Please download the ViViD checkpoints from their official Hugging Face page (linked in the GitHub repo)\n",
        [span_22](start_span)"# and place them in the 'ViViD/checkpoints/' directory as per their instructions.[span_22](end_span)\n",
        "print(\"ACTION REQUIRED: Manually download ViViD weights into ViViD/checkpoints/\")\n",
        "\n",
        "# --- vid2densepose Checkpoint ---\n",
        [span_23](start_span)"# This is the model for DensePose extraction.[span_23](end_span)\n",
        "!wget -P vid2densepose/model/ https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
        "\n",
        "# --- Segment Anything (SAM) Checkpoint ---\n",
        [span_24](start_span)[span_25](start_span)"# Using the ViT-H SAM model.[span_24](end_span)[span_25](end_span)\n",
        "!wget -P segment-anything/checkpoints/ https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "# --- Human Parsing Checkpoint (for Agnostic Video) ---\n",
        [span_26](start_span)"# This model is used by OOTDiffusion's preprocessing steps.[span_26](end_span)\n",
        "!wget -P Self-Correction-Human-Parsing/checkpoints/ https://github.com/GoGoDuck912/Self-Correction-Human-Parsing/releases/download/eccv2022/exp-schp-201908261155-lip.pth\n",
        "\n",
        "print(\"\\nDownloads complete. Please ensure you manually downloaded the ViViD weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configuration\n",
        "\n",
        "Set the paths for your input files and the directory for the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- USER INPUTS ---\n",
        "\n",
        "# Path to the input video of the person\n",
        "# IMPORTANT: The person video should be a short clip (e.g., 5-10 seconds).\n",
        "person_video_path = \"./path/to/your/person_video.mp4\" \n",
        "\n",
        "# Path to the input image of the garment\n",
        "garment_image_path = \"./path/to/your/garment_image.png\"\n",
        "\n",
        "# Directory to save all intermediate and final results\n",
        "output_dir = \"./vton_results\"\n",
        "\n",
        "# --- END USER INPUTS ---\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        [span_27](start_span)"# Define paths for all intermediate files from the master script concept[span_27](end_span)\n",
        "densepose_video_path = os.path.join(output_dir, \"densepose.mp4\")\n",
        "garment_mask_path = os.path.join(output_dir, \"garment_mask.png\")\n",
        "agnostic_video_path = os.path.join(output_dir, \"agnostic.mp4\")\n",
        "final_video_path = os.path.join(output_dir, \"result.mp4\")\n",
        "\n",
        "print(f\"Input Person Video: {person_video_path}\")\n",
        "print(f\"Input Garment Image: {garment_image_path}\")\n",
        "print(f\"Output Directory: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: The VTON Pipeline\n",
        "Here, we define Python functions to encapsulate each stage of the pipeline described in the report. [span_28](start_span)This automates the series of command-line calls and script executions.[span_28](end_span)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import yaml\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "def generate_densepose_video(input_video, output_video):\n",
        [span_29](start_span)[span_30](start_span)"    \"\"\"Runs the vid2densepose script to extract human pose.[span_29](end_span)[span_30](end_span)\"\"\"\n",
        "    print(\"\\n--- Stage 1: Generating DensePose Video ---\")\n",
        "    vid2densepose_script = \"vid2densepose/main.py\"\n",
        "    command = [\n",
        "        \"python\", vid2densepose_script,\n",
        "        \"-i\", input_video,\n",
        "        \"-o\", output_video,\n",
        "        \"--model_path\", \"vid2densepose/model/model_final_162be9.pkl\"\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(\"✅ DensePose video generated successfully.\")\n",
        "\n",
        "def generate_garment_mask(input_image, output_mask):\n",
        [span_31](start_span)[span_32](start_span)[span_33](start_span)"    \"\"\"Uses Segment Anything Model (SAM) to isolate the garment.[span_31](end_span)[span_32](end_span)[span_33](end_span)\"\"\"\n",
        "    print(\"\\n--- Stage 2: Generating Garment Mask ---\")\n",
        "    sam_checkpoint = \"segment-anything/checkpoints/sam_vit_h_4b8939.pth\"\n",
        "    model_type = \"vit_h\"\n",
        "    device = \"cuda\"\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    sam.to(device=device)\n",
        "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "    image = cv2.imread(input_image)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image file: {input_image}\")\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    masks = mask_generator.generate(image_rgb)\n",
        "\n",
        [span_34](start_span)"    # Find the best mask - typically the one with the largest area[span_34](end_span)\n",
        "    if not masks:\n",
        "        raise ValueError(\"SAM did not find any objects in the garment image.\")\n",
        "    \n",
        "    sorted_masks = sorted(masks, key=lambda m: m['area'], reverse=True)\n",
        "    best_mask = sorted_masks[0]['segmentation']\n",
        "    \n",
        [span_35](start_span)"    # Create a binary image (0 or 255) and save it[span_35](end_span)\n",
        "    binary_mask_img = (best_mask * 255).astype(np.uint8)\n",
        "    cv2.imwrite(output_mask, binary_mask_img)\n",
        "    print(\"✅ Garment mask generated successfully.\")\n",
        "\n",
        "def generate_agnostic_video(input_video, output_video):\n",
        "    \"\"\"\n",
        [span_36](start_span)"    Generates the clothing-agnostic video.[span_36](end_span)\n",
        [span_37](start_span)[span_38](start_span)"    This is a complex step involving human parsing and inpainting.[span_37](end_span)[span_38](end_span)\n",
        [span_39](start_span)"    The ViViD paper recommends the method from OOTDiffusion.[span_39](end_span)\n",
        "    This function calls the OOTDiffusion preprocessing script.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Stage 3: Generating Clothing-Agnostic Video ---\")\n",
        "    agnostic_script = \"OOTDiffusion/run_preprocess.py\"\n",
        "    \n",
        "    if not os.path.exists(agnostic_script):\n",
        "        print(\"⚠️ WARNING: Agnostic video generation script not found.\")\n",
        "        print(f\"This step is being SKIPPED. Please ensure '{agnostic_script}' exists.\")\n",
        "        return\n",
        "        \n",
        "    # This is a conceptual call. The actual OOTDiffusion script is designed for single images.\n",
        "    # You would need to adapt it to process a video frame-by-frame.\n",
        "    # For this demo, we will show the command for a single frame as an example.\n",
        "    print(\"NOTE: The following command is a template for frame-by-frame processing.\")\n",
        "    print(\"A real implementation requires a wrapper script to handle videos.\")\n",
        "    # Example command (conceptual):\n",
        "    # subprocess.run([\"python\", agnostic_script, \"...frame-by-frame-logic...\"], check=True)\n",
        "    print(\"SKIPPING agnostic video generation. A wrapper script is required.\")\n",
        "    \n",
        "def run_vivid_inference(config_path, person_video, densepose_video, garment_image, garment_mask, final_output_dir):\n",
        [span_40](start_span)"    \"\"\"Runs the final ViViD model inference.[span_40](end_span)\"\"\"\n",
        "    print(\"\\n--- Stage 4: Running ViViD Inference ---\")\n",
        "    \n",
        "    # 4a. Modify the ViViD config file dynamically\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        [span_41](start_span)"    # Update paths in the config[span_41](end_span)\n",
        "    config['test_cases']['upper_1']['garment_path'] = os.path.abspath(garment_image)\n",
        "    config['test_cases']['upper_1']['garment_mask_path'] = os.path.abspath(garment_mask)\n",
        "    config['test_cases']['upper_1']['video_path'] = os.path.abspath(person_video) # The agnostic video path\n",
        "    config['test_cases']['upper_1']['pose_video'] = os.path.abspath(densepose_video)\n",
        "    config['out_dir'] = os.path.abspath(final_output_dir)\n",
        "    \n",
        "    modified_config_path = os.path.join(output_dir, \"run_config.yaml\")\n",
        "    with open(modified_config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    print(f\"Modified config saved to {modified_config_path}\")\n",
        "\n",
        "    # 4b. Run the inference script\n",
        "    vivid_inference_script = \"ViViD/vivid.py\"\n",
        "    command = [\n",
        "        \"python\", vivid_inference_script,\n",
        "        \"--config\", modified_config_path\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(f\"✅ Final video generated in directory: {final_output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Orchestration and Execution\n",
        "\n",
        [span_42](start_span)"Now, we'll create a master function to run the entire pipeline in sequence and then execute it.[span_42](end_span)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_full_pipeline():\n",
        "    \"\"\"Orchestrates the entire VTON pipeline from raw inputs to final video.\"\"\"\n",
        "    print(\"===========================================\")\n",
        "    print(\"STARTING VIDEO VIRTUAL TRY-ON PIPELINE\")\n",
        "    print(\"===========================================\")\n",
        "\n",
        "    try:\n",
        [span_43](start_span)"        # Stage 1: DensePose Extraction[span_43](end_span)\n",
        "        generate_densepose_video(person_video_path, densepose_video_path)\n",
        "\n",
        [span_44](start_span)"        # Stage 2: Garment Masking[span_44](end_span)\n",
        "        generate_garment_mask(garment_image_path, garment_mask_path)\n",
        "\n",
        [span_45](start_span)"        # Stage 3: Clothing-Agnostic Video[span_45](end_span)\n",
        "        # NOTE: This is the most complex step. As noted in the function above, a real implementation\n",
        "        # requires a custom script to process video frames with OOTDiffusion's tools.\n",
        "        # For this demo to run, we will pass the ORIGINAL person video to ViViD. This will lead\n",
        [span_46](start_span)"        # to artifacts from the original clothing 'leaking' through.[span_46](end_span)\n",
        "        print(\"\\n--- Stage 3: Generating Clothing-Agnostic Video (SKIPPED) ---\")\n",
        "        print(\"Using original person video as the agnostic video. For best results, implement this step.\")\n",
        "        agnostic_input_video = person_video_path \n",
        "\n",
        "        # Stage 4: Run ViViD Inference\n",
        "        vivid_config_template = \"ViViD/configs/prompts/upper1.yaml\"\n",
        "        vivid_output_dir = os.path.join(output_dir, 'vivid_output')\n",
        "        run_vivid_inference(\n",
        "            config_path=vivid_config_template, \n",
        "            person_video=agnostic_input_video, \n",
        "            densepose_video=densepose_video_path, \n",
        "            garment_image=garment_image_path, \n",
        "            garment_mask=garment_mask_path, \n",
        "            final_output_dir=vivid_output_dir\n",
        "        )\n",
        "        \n",
        "        print(\"\\n===========================================\")\n",
        "        print(\"🎉 PIPELINE COMPLETED SUCCESSFULLY! 🎉\")\n",
        "        print(f\"Check the '{vivid_output_dir}' directory for your video.\")\n",
        "        print(\"===========================================\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\nERROR: A required file was not found. Please check your paths.\")\n",
        "        print(f\"Is '{person_video_path}' or '{garment_image_path}' correct?\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\nERROR: A script failed to execute. See the output above for details.\")\n",
        "        print(\"This is often due to out-of-memory errors on the GPU.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "\n",
        "# --- UNCOMMENT THE LINE BELOW TO RUN THE PIPELINE ---\n",
        "# Make sure you have set the input file paths in Step 3.\n",
        "# run_full_pipeline()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
